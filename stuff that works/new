import sklearn
import pandas as pd
from sklearn.model_selection import train_test_split

dataset = pd.read_csv('numbers.csv')

#print(dataset)

"""
# split the data set for training/testing
def LinearRegressionModel(dataset):
    X = dataset[['AT', 'V', 'AP', 'RH']]
    y = dataset['PE']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # test size = 20% alalocation random state is the rest otherwise known as the pareto priniciple but i dont fucking know

    #Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)
    #Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain, Ytrain, test_size=0.2, random_state=0)



    # we must know instantiate linear regression and train our fucking demonbaby
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(X_train, y_train)
    # interpret our demonbaby
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

    y_prediction = model.predict(X_test)
    mse = mean_squared_error(y_test, y_prediction)
    mae = mean_absolute_error(y_test, y_prediction)
    r2 = r2_score(y_test, y_prediction)
    print("LINEAR REGRESSION: ")

    print("mse = ", mse)
    print("mae = ", mae)
    print("Pirate Noise Squared = ", r2)

    coefficients = model.coef_
    intercept = model.intercept_

    print("Coefficents: ",coefficients)
    print("Intercept:",intercept)
    print("---------------------------------")

    """

#def GradiantBoostingRegressionModel(dataset):
#split the dataset, axis= 1 in pandas refers to the column, where drop lets us isolate values.
Xs = dataset.drop('PE',axis=1)
ys= dataset['PE']

#we do the same shit as linear regression according to docs for training
X_train, X_test, y_train, y_test = train_test_split(Xs,ys, test_size=0.2, random_state=80)
from sklearn.ensemble import GradientBoostingRegressor
ergoRegression = GradientBoostingRegressor(n_estimators=100, learning_rate=0.2, max_depth=3, random_state=80)
ergoRegression.fit(X_train, y_train) 

y_prediction = ergoRegression.predict(X_test)

# make a dictionary of hyperparameter values to search
search_space = {
    "loss": ["squared_error"],
    "learning_rate": [0.001,0.01,0.1,1],
    "n_estimators": [1,10,100,500],
    "max_depth": [2,4,8,16],
    "min_samples_split": [2,4,8],
    "min_samples_leaf": [10,50,100],
}

from sklearn.model_selection import GridSearchCV
# make a GridSearchCV object

GS = GridSearchCV(estimator =  GradientBoostingRegressor(),
                  param_grid = search_space,
                  scoring = ["r2", "neg_mean_squared_error"], #sklearn.metrics.SCORERS.keys()
                  refit = "r2",
                  cv = 5,
                  verbose = 4)

GS.fit(X_test, y_test)

print("best model: ", GS.best_estimator_) # to get the complete details of the best model
print("best parameter values: ", GS.best_params_) # to get only the best hyperparameter values that we searched for
print("best r^2 value: ", GS.best_score_) # score according to the metric we passed in refit

df = pd.DataFrame(GS.cv_results_)
df = df.sort_values("rank_test_r2")
df.to_csv("cv_gbr_results.csv", index = False)



"""   from sklearn.metrics import mean_squared_error, r2_score

  mse = mean_squared_error(y_test, y_prediction)
  pirateSoundSquared = r2_score(y_test, y_prediction)
  print("GRADIANT REGRESSION: ")
  print("Mean Squared Error:", mse)
  print("R2 Score:", pirateSoundSquared)


  print("MSE:, " ,mse)
  print("R2, ", pirateSoundSquared) """

#LinearRegressionModel(dataset)
#GradiantBoostingRegressionModel(dataset)


